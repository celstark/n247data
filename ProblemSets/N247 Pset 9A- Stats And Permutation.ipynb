{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"DViHHKVrfFRu"},"source":["# Overview\n","If you've taken any kind of stats course, you've come across things like t-tests, ANOVAs, and other members of the GLM family. You should know that they return p-values, or the odds that your sample (or samples) were drawn from noise. So, if you have two sets of numbers, setA and setB, the p-value refers to the odds that you'd observe a difference like the one you did (or larger) if they were, in fact, drawn from the same distribution. If the null were true (and they're from the same pool), you'd come up with your data (or more extreme) p-value percent of the time.   \n","\n","Now, this leads to two types of errors you'd also heard about. Type I errors are often the most feared as those are \"false positives\" - the truth is there is no difference, but you still observed one.  Type II are \"false negatives\" - something was there, but you didn't see it. We're taught a few things about the GLM and one of the big ones is that the assumption that your data are \"normally distributed\". If they are, the test is valid. If not, the test isn't valid and you should either fix your data (transform it to make it normally distributed) or look elsewhere (say to a non-parametric test in classical frequentist statistics). \n","\n","In this project (Problem Set 9A), we're going to play around a bit with this and see just what happens when these assumptions are violated.  It'll give us a good bit of practice in simulating data and in running statistical tests. It'll also let us see that the t-test (and its GLM kin) are actually quite robust to Type I errors when normality is violated. It's not, however, robust to Type II errors. If you want to read up on this effect, here's a \n","[nice paper on this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3139856/).\n","\n","In the next one (9B), we'll code up a permutation test that computes the probability via brute force. These are getting more and more popular now that we have fast computers and don't do our statisitcs on ink quill pens and parchment.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Eh2ttq1wG22M"},"source":["First, write a function `GenNormalSamples` that generates two random sets of numbers using `scipy.stats.norm`. Parameters to the function should include the number of samples per group (`nsamples` defaulting to 10), the mean and standard deviation of setA (`meanA` defaulting to 0 and `stdA` defaulting to 1) and the same for setB (`meanB` and `stdB`). Numbers should be drawn from a normal random (Gaussian) distribution.  Return the `nsamples`x2 numpy array.\n","\n","To check this out, use `plt.hist` to plot both your histograms on the same axes of the results from `GenNormalSamples(100,meanB=1.0)`.  It won't look exactly like this, but should be something like it.\n","\n","![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig1.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG97HTadH1bX"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import norm\n","from matplotlib import pyplot as plt\n","def GenNormalSamples(nsamples=10, meanA=0.0, stdA=1.0, meanB=0.0, stdB=1.0):\n","    # Your code here...\n","    return result\n","\n","data=GenNormalSamples(100,meanB=1.0)\n","plt.hist(data)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bnmmr4pHKnZ1"},"source":["Next, write a function `CheckTtest` that calls `GenNormalSamples` `ntest` times (defult=1000) to generate random data and then calls `scipy.stats.ttest_ind` each time to test whether a standard t-test thinks these samples, in fact, come from different distributions. This function should take the same `nsamples`, `meanA/B` and `stdA/B` variables as `GenNormalSamples` and should pass these through to `GenNormalSamples`. \n","\n","From each call, keep track of the p-value returned. At the end, print out a nice statement that says \"Out of ## tests, ## were significant, for a test positivity rate of 0.##\". Use a p\\<0.05 threshold as your \"significant\" rate.\n","\n","Run your test with the default values (it should return roughly 0.05) and also with a few values of your choosing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pin36j1MpaG"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import ttest_ind\n","\n","def CheckTtest(ntests=1000,nsamples=10,meanA=0,stdA=1,meanB=0,stdB=1):\n","    # Your code here...\n","\n","CheckTtest()\n","CheckTtest(meanB=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dNxotdcsQ5uB"},"source":["Now, copy your `GenNormalSamples` function and re-write it to create a skewed sample. Instead of using `scipy.stats.norm`, you'll use `scipy.stats.exponnorm`. \n","\n","This function takes a K parameter that controls the skew. K here must be >0, but 0 would be an actually un-skewed distribution. So, make sure to clip whatever K is passed in to a very small number like 0.0001 to approximate an unskewed normal distribution well enough.  Set the default `k` to be 0 so that by default, this does make a \"normal\" distribution sample.\n","\n","\n","Make the histogram you did as before, but use a k=20 this time.  It won't look exactly like this, but should be close:\n","\n","![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig2.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1QH0hMsR6G4"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import exponnorm\n","from matplotlib import pyplot as plt\n","def GenNormalSamples(nsamples=10, meanA=0.0, stdA=1.0, meanB=0.0, stdB=1.0,k=0.0):\n","    # Your code here...\n","\n","data=GenNormalSamples(100,meanB=1.0,k=20.0)\n","plt.hist(data)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Sn4BVYrJVfpq"},"source":["Copy your `CheckTtests` in here as well also adjust it to take the `k` parameter. Run it as before. What happens to the test positive rates when you use a value like k=20? Does it behave according to how we want it to behave?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGs7FPguU8Kr"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","\n","def CheckTtest(ntests=1000,nsamples=10,meanA=0,stdA=1,meanB=0,stdB=1,k=0):\n","    # Your code here\n","\n","print('These should be low (looking for Type I, want low) - same means - no skew and skew')\n","CheckTtest()\n","CheckTtest(k=20)\n","\n","print('\\nShifting the meanB to be 1.0 (looking for Type II, want high) - no skew and skew')\n","CheckTtest(meanB=1)\n","CheckTtest(meanB=1,k=20)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"utMY_M6LWbIB"},"source":["Clearly, the t-test is having an issue when the data aren't normally distributed (you do check all your data for normality when you use t-tests in your research right? *Right???*). Traditional stats have lots of options for you here with things like transforming your data to be normal or using non-parametric tests. \n","\n","But, we have computers and computers can do math fast. Why use tests that assume normality to estimate a probability when you can brute force the probability?  That's the goal we'll take in 9B...\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO2KGv9QQtChYiQrTUO5gny","provenance":[{"file_id":"1NFIbl7M1bvmWA_vT9I0sRMQkRzmvRJXt","timestamp":1679007859208}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
