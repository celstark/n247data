{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"utMY_M6LWbIB"},"source":["# Overview\n","\n","We left off in 9.1 with our t-tests having issues with non-normal data. Traditional stats have lots of options for you here with things like transforming your data to be normal or using non-parametric tests. \n","\n","But, we have computers and computers can do math fast. Why use tests that assume normality to estimate a probability when you can brute force the probability?  Remember, your standard statistical tests are returning a p-value that is trying to get at the odds this could have happneed by chance. So, if you drew `nsamples` from setA and `nsamples` from setB, it's trying (by assuming a shape of a distribution) to calculate, if they really came from the same pool, how often you'd observe data as cool as your data.\n","\n","We take this logic then and exploit it a bit. You have some set of observations and you know which came from setA and which from setB. You think that that secret-decoder-ring is cool (the decoder that assign certain numbers to setA and others to setB). A **permutation test** says \"If I permuted my labels to generate every possible setA and setB from my data (all possible re-labeling of points) and computed the difference between means each time, I could determine just how cool my set of lables (secret decoder ring) really is.\". So, if your assignment of data points to setA vs. setB is better than 99.9% of all possible assignments, that makes yours pretty cool - pretty darn unlikely to occur by chance. In fact, your odds of something as cool as your set of labels or cooler is 0.001 in that case. Exactly that. It's an exact test of the probability we're after.\n","\n","When sample sizes are modest, we can run full permutations. Here's a bit of code that will compute just how many are needed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1678850035662,"user":{"displayName":"Craig Stark","userId":"11041610993007874858"},"user_tz":420},"id":"0yGCpKREnGq8","outputId":"04be6255-9c6c-4df7-f152-96e8079a9511"},"outputs":[{"name":"stdout","output_type":"stream","text":["We have 10 in one group and 10 in the other\n","We then have a total of 184756 combinations\n","Or, computed another way ... (hint hint) 184756 combinations\n"]}],"source":["import itertools\n","import numpy as np\n","from math import factorial\n","\n","npergroup=10\n","ntotal=npergroup*2\n","groups=np.mod(np.arange(ntotal),2)\n","print(f\"We have {np.sum(groups==0)} in one group and {np.sum(groups==1)} in the other\")\n","ncombos=factorial(ntotal) / (factorial(npergroup) * factorial(ntotal-npergroup))\n","print(f\"We then have a total of {ncombos:.0f} combinations\")\n","\n","all_combos=list(itertools.combinations(groups,npergroup))\n","print(f\"Or, computed another way ... (hint hint) {len(all_combos)} combinations\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vydY-uC0nygD"},"source":["# Get the data\n","For this next bit, we're going to use some datasets I've already made. I used my code for `GenNormalSamples` to make up some data for us, so that we're all working on the same input. I've got a few ways you can get / load the data for this.  First, you can just download it from the class [GitHub repository](https://github.com/celstark/n247data). It's the Orig*.csv set there.  If you're working in a local Jupyter notebook, this may be the easiest way. Stick that in your folder here and you can use the normal Pandas functions to read CSV files just fine.\n","\n","Second, though, you can save these files into a Google Drive and load them from here. This is particularly relevant if you're running things on Google Colab.  Third, you can pull them straight from the GitHub repo as you run the code.  \n","\n","Samples of how to do the last two options are here. Note `np.loadtxt` and `np.genfromtxt` only differ in how they handle missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2103,"status":"ok","timestamp":1678849469837,"user":{"displayName":"Craig Stark","userId":"11041610993007874858"},"user_tz":420},"id":"gpPUGKDEozMn","outputId":"bc80da7d-a327-490b-fd68-0e33c56d675c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# Loading the files from your Google Drive if you're using Colab and have copied files to your drive (edit as needed to change the actual location)\n","from google.colab import drive\n","import os\n","import numpy as np\n","drive.mount('/content/gdrive')\n","# This can be a personal drive, a shared drive, etc.  Point it to the folder here\n","studydir='/content/gdrive/My Drive/N247_2023'\n","os.chdir(studydir)  \n","\n","OrigSkew=np.loadtxt('OrigSkew.csv')\n","OrigNorm=np.loadtxt('OrigNorm.csv')\n","OrigNullSkew=np.loadtxt('OrigNullSkew.csv')\n","OrigNullNorm=np.loadtxt('OrigNullNorm.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"B7s_oAu2o7Hw"},"outputs":[],"source":["# Loading the files from GitHub directly\n","import numpy as np\n","\n","OrigNorm = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigNorm.csv')\n","OrigSkew = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigSkew.csv')\n","OrigNullNorm = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigNullNorm.csv')\n","OrigNullSkew = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigNullSkew.csv')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ES3BRdGcpuse"},"source":["Now that you've got the data, you're going to write both a permutation test and a Monte Carlo analysis of the data. As described above, permutation tests run all possible permutations of the groupings and are great when that number isn't crazy. When that number gets to be too big, we approximate it via a **Monte Carlo** analysis. This just means, we do a random sampling of those permutations and, if that sampling is really random and of a decent size, we can estimate our proabability just fine.  It sounds fancy, but yea - it's just a random sampling of the permutations.\n","\n","For this next bit, though, we only have 185k possible permutations and this should take just a few seconds to run them all.  So, in this next bit, you'll write a function `PermutationTest` that iterates over all permutations of putting half (10) of the observations in one group and the other half (10) in the other group.  So, for each iteration, compute the mean in each group, compute the difference between these, and store that in an array. If you want this to run before you graduate, make sure to pre-allocate your results array and don't use `append` each time through your loop.\n","\n","At the end, return the percentile for the original group-assignment difference. If, for example, your original difference in means was 5 and there were 2340 values in your \"all differences\" array that were greater than 5, the percentile would be 2340/184756 or 0.0127 when considered \"one-tailed\".  To make this a two-tailed test, you'd take the absolute value of the original difference and count how many times the absolute value of your \"all differences\" array was greater than this, dividing again by the total number of permutations.  (Note, when doing things one-tailed, consider handling positive and negative original differences intelligently).\n","\n","Here are target numbers for the *OrigNullNorm* dataset:\n","```\n","Original difference 0.30\n","t-test p=0.563\n","Permutaton p=0.559\n","```\n","Run on all 4 datasets (*OrigNullNorm*, *OrigNullSkew*, *OrigNorm*, and *OrigSkew*). When getting going, you'll want to comment out all but one of the actual analysis bits.  The numbers above for OrigNullNorm should help you know if you're working. I've included a little plot routine as well that lets you look at the distribution of differences. This is also my plot for the *OrigNullNorm* and *OrigSkew* datasets.\n","\n","| ![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig3a.png)       | ![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig3b.png)    | \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQi956E4wOH7"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","import itertools\n","from math import factorial\n","import numpy as np\n","import seaborn as sns\n","\n","def PermutationPlot(all_diffs, orig_diff=None):\n","    # Handy function to plot the output of your PertmutationTest call\n","    g=sns.displot(all_diffs,kind='kde')\n","    if orig_diff:\n","        g.refline(x=orig_diff,color='k')\n","\n","def PermutationTest(data,tails=2,verbose=False):\n","    # Your code here...\n","   \n","    return ptile,orig_diff,all_diffs\n","\n","\n","# Start by commenting out all but one of these analyses until you've got things going.\n","ptile,orig_diff,all_diffs=PermutationTest(OrigNullNorm)\n","print(f'Null, normal p={ptile:.3f}\\n')\n","PermutationPlot(all_diffs,orig_diff)\n","\n","ptile,orig_diff,all_diffs=PermutationTest(OrigNullSkew)\n","print(f'Null effect, skew p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=PermutationTest(OrigNorm)\n","print(f'Effect, normal p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=PermutationTest(OrigSkew)\n","print(f'Effect, skew p={ptile:.3f}\\n')\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6ZVlsPikVWY9"},"source":["# Monte Carlo version\n","Well, 185k isn't so crazy for this kind of thing, but what if we needed a lot more math than just a simple difference? What if each iteration then took several seconds?  Or, what if we have larger samples than two groups of 10?  The time it takes to run a full permutation can get out of hand quickly.\n","\n","Just as we can approximate how a population behaves by sampling it (the fundamental idea in all of statistics), we can approximate how our measure behaves by sampling it rather than by testing every possible point. That's what we do in a Monte Carlo analysis.\n","\n","Here, re-write your code to just randomly shuffle the groups `nshuffles` times (default 1000), compute the difference each time and return the same percentile and \"all differences\" array as before. This should be a plug-in replacement for your `PermutationTest()` routine.  Run on all 4 datasets and you should see the p-values are darn close, even with the default of 1000 samples.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ig2QvyieWgpn"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","import numpy as np\n","\n","def MonteCarloTest(data,nshuffles=1000,tails=2,verbose=False):\n","    # Your code here\n","\n","    return ptile,orig_diff,all_diffs\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigNullNorm)\n","print(f'Null, normal p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigNullSkew)\n","print(f'Null effect, skew p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigNorm)\n","print(f'Effect, normal p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigSkew)\n","print(f'Effect, skew p={ptile:.3f}\\n')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO2KGv9QQtChYiQrTUO5gny","provenance":[{"file_id":"1NFIbl7M1bvmWA_vT9I0sRMQkRzmvRJXt","timestamp":1679007859208}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
