{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"N247 PSet Pandas 4 - Permutation analysis.ipynb","provenance":[],"authorship_tag":"ABX9TyMi4lB57BvbR1CaPmmevqjs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bo9tMuxy1EhT"},"source":["In the last problem set, we ran t-tests and, when we noticed things were non-normal, ran a Mann-Whitney U test.  There's another approach that is often useful, known as a permutation approach (and its approximation, the Monte Carlo analysis).  Like other tests, the idea is that you've got, for example, two groups (active and control) and a measure on each with the goal of determining whether the groups differ on that measure. In the permutation approach, rather than assuming some shape of a null distribution for that difference score, you derive it by shuffling the labels.  The question becomes, does your true assignment of labels (which are \"active\" and which are \"control\") come up with a bigger difference than you'd get if you randomly picked labels of \"active\" and \"control\"?  Put another way, if you ran 10,000 or so random shuffles of the labels, how often would you come up with a mean difference between groups as extreme or more extreme than the difference between groups using your labels?\n","\n","We're going to use the same dataset as we've used in the last few assignments.  Here, though, I'd like you to write a function that will:\n","\n","- Take as its input several `pd.Series` objects (just to mix things up)\n","  - A series (data) with your actual measure\n","  - A series that has True/False values for membership in _set1_\n","  - Likewise but membership in set2.  Note, there can be observations that are in your data but are not in either _set1_ or _set2_\n","  - An optional _nperms_ input that defaults to 1000 but controls the number of permutations run\n","- Determine the actual difference in the measure between _set1_ and _set2_\n","- Runs _nperms_ permutations by shuffling subjects and randomly assigning these subjects to _set1_ or _set2_. Note, you must use only those in _set1_ and _set2_ and cannot assume that the total list is _set1+set2_ (i.e., these are just partial subsets of the full \"data\".) Figures the mean difference\n","for each of these shuffled _set1_ vs. _set2_\n","- Computes how often the a randomly shuffled difference is ***more extreme*** than the observed difference. That is, how often is the absolute value of a random difference greater than the absolute value of the observed  difference?\n","- Returns: The probability that random shuffles will come up with a difference more extreme than the observed difference between groups\n","\n","Use code like this to define your measures and a sample pair of sets:\n","\n","```\n","age=data['Age']\n","LDI=data['LDI']\n","\n","# Create 2 sample groups -- True/False values for being in each group\n","set1=age<40\n","set2=(age>40) & (age <=60)\n","```\n","***What probabilities do you come up with for a call like:***\n","```\n","permutation_test(LDI, set1, set2)\n","permutation_test(RAVLT, set1, set2)\n","```\n","**Bonus**: Have your function optionally draw the histogram of values it came up with and mark on there where your observed difference really lies.\n","\n"," "]},{"cell_type":"code","metadata":{"id":"6f9ZOrII1C3R"},"source":[""],"execution_count":null,"outputs":[]}]}