{"cells":[{"cell_type":"markdown","metadata":{"id":"DViHHKVrfFRu"},"source":["If you've taken any kind of stats course, you've come across things like t-tests, ANOVAs, and other members of the GLM family. You should know that they return p-values, or the odds that your sample (or samples) were drawn from noise. So, if you have two sets of numbers, setA and setB, the p-value refers to the odds that you'd observe a difference like the one you did (or larger) if they were, in fact, drawn from the same distribution. If the null were true (and they're from the same pool), you'd come up with your data (or more extreme) p-value percent of the time.   \n","\n","Now, this leads to two types of errors you'd also heard about. Type I errors are often the most feared as those are \"false positives\" - the truth is there is no difference, but you still observed one.  Type II are \"false negatives\" - something was there, but you didn't see it. We're taught a few things about the GLM and one of the big ones is that the assumption that your data are \"normally distributed\". If they are, the test is valid. If not, the test isn't valid and you should either fix your data (transform it to make it normally distributed) or look elsewhere (say to a non-parametric test in classical frequentist statistics). \n","\n","In this project, we're going to play around a bit with this and see just what happens when these assumptions are violated.  It'll give us a good bit of practice in simulating data and in running statistical tests. It'll also let us see that the t-test (and its GLM kin) are actually quite robust to Type I errors when normality is violated. It's not, however, robust to Type II errors. If you want to read up on this effect, here's a \n","[nice paper on this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3139856/).\n","\n","At the end, we'll code up a permutation test that computes the probability via brute force. These are getting more and more popular now that we have fast computers and don't do our statisitcs on ink quill pens and parchment.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Eh2ttq1wG22M"},"source":["First, write a function `GenNormalSamples` that generates two random sets of numbers using `scipy.stats.norm`. Parameters to the function should include the number of samples per group (`nsamples` defaulting to 10), the mean and standard deviation of setA (`meanA` defaulting to 0 and `stdA` defaulting to 1) and the same for setB (`meanB` and `stdB`). Numbers should be drawn from a normal random (Gaussian) distribution.  Return the `nsamples`x2 numpy array.\n","\n","To check this out, use `plt.hist` to plot both your histograms on the same axes of the results from `GenNormalSamples(100,meanB=1.0)`.  It won't look exactly like this, but should be something like it.\n","\n","![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig1.png)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QG97HTadH1bX"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import norm\n","from matplotlib import pyplot as plt\n","def GenNormalSamples(nsamples=10, meanA=0.0, stdA=1.0, meanB=0.0, stdB=1.0):\n","    # Your code here...\n","    return result\n","\n","data=GenNormalSamples(100,meanB=1.0)\n","plt.hist(data)\n"]},{"cell_type":"markdown","metadata":{"id":"bnmmr4pHKnZ1"},"source":["Next, write a function `CheckTtest` that calls `GenNormalSamples` `ntest` times (defult=1000) to generate random data and then calls `scipy.stats.ttest_ind` each time to test whether a standard t-test thinks these samples, in fact, come from different distributions. This function should take the same `nsamples`, `meanA/B` and `stdA/B` variables as `GenNormalSamples` and should pass these through to `GenNormalSamples`. \n","\n","From each call, keep track of the p-value returned. At the end, print out a nice statement that says \"Out of ## tests, ## were significant, for a test positivity rate of 0.##\". Use a p\\<0.05 threshold as your \"significant\" rate.\n","\n","Run your test with the default values (it should return roughly 0.05) and also with a few values of your choosing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pin36j1MpaG"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import ttest_ind\n","\n","def CheckTtest(ntests=1000,nsamples=10,meanA=0,stdA=1,meanB=0,stdB=1):\n","    # Your code here...\n","\n","CheckTtest()\n","CheckTtest(meanB=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dNxotdcsQ5uB"},"source":["Now, copy your `GenNormalSamples` function and re-write it to create a skewed sample. Instead of using `scipy.stats.norm`, you'll use `scipy.stats.exponnorm`. \n","\n","This function takes a K parameter that controls the skew. K here must be >0, but 0 would be an actually un-skewed distribution. So, make sure to clip whatever K is passed in to a very small number like 0.0001 to approximate an unskewed normal distribution well enough.  Set the default `k` to be 0 so that by default, this does make a \"normal\" distribution sample.\n","\n","\n","Make the histogram you did as before, but use a k=20 this time.  It won't look exactly like this, but should be close:\n","\n","![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig2.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1QH0hMsR6G4"},"outputs":[],"source":["import numpy as np\n","from scipy.stats import exponnorm\n","from matplotlib import pyplot as plt\n","def GenNormalSamples(nsamples=10, meanA=0.0, stdA=1.0, meanB=0.0, stdB=1.0,k=0.0):\n","    # Your code here...\n","\n","data=GenNormalSamples(100,meanB=1.0,k=20.0)\n","plt.hist(data)\n"]},{"cell_type":"markdown","metadata":{"id":"Sn4BVYrJVfpq"},"source":["Copy your `CheckTtests` in here as well also adjust it to take the `k` parameter. Run it as before. What happens to the test positive rates when you use a value like k=20? Does it behave according to how we want it to behave?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGs7FPguU8Kr"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","\n","def CheckTtest(ntests=1000,nsamples=10,meanA=0,stdA=1,meanB=0,stdB=1,k=0):\n","    # Your code here\n","\n","print('These should be low (looking for Type I, want low) - same means - no skew and skew')\n","CheckTtest()\n","CheckTtest(k=20)\n","\n","print('\\nShifting the meanB to be 1.0 (looking for Type II, want high) - no skew and skew')\n","CheckTtest(meanB=1)\n","CheckTtest(meanB=1,k=20)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"utMY_M6LWbIB"},"source":["Clearly, the t-test is having an issue when the data aren't normally distributed (you do check all your data for normality when you use t-tests in your research right? *Right???*). Traditional stats have lots of options for you here with things like transforming your data to be normal or using non-parametric tests. \n","\n","But, we have computers and computers can do math fast. Why ue tests that assume normality to estimate a probability when you can brute force the probability?  Remember, your standard statistical tests are returning a p-value that is trying to get at the odds this could have happneed by chance. So, if you drew `nsamples` from setA and `nsamples` from setB, it's trying (by assuming a shape of a distribution) to calculate, if they really came from the same pool, how often you'd observe data as cool as your data.\n","\n","We take this logic then and exploit it a bit. You have some set of observations and you know which came from setA and which from setB. You think that that secret-decoder-ring is cool (the decoder that assign certain numbers to setA and others to setB). A **permutation test** says \"If I permuted my labels to generate every possible setA and setB from my data (all possible re-labeling of points) and computed the difference between means each time, I could determine just how cool my set of lables (secret decoder ring) really is.\". So, if your assignment of data points to setA vs. setB is better than 99.9% of all possible assignments, that makes yours pretty cool - pretty darn unlikely to occur by chance. In fact, your odds of something as cool as your set of labels or cooler is 0.001 in that case. Exactly that. It's an exact test of the probability we're after.\n","\n","When sample sizes are modest, we can run full permutations. Here's a bit of code that will compute just how many are needed."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1678850035662,"user":{"displayName":"Craig Stark","userId":"11041610993007874858"},"user_tz":420},"id":"0yGCpKREnGq8","outputId":"04be6255-9c6c-4df7-f152-96e8079a9511"},"outputs":[{"name":"stdout","output_type":"stream","text":["We have 10 in one group and 10 in the other\n","We then have a total of 184756 combinations\n","Or, computed another way ... (hint hint) 184756 combinations\n"]}],"source":["import itertools\n","import numpy as np\n","from math import factorial\n","\n","npergroup=10\n","ntotal=npergroup*2\n","groups=np.mod(np.arange(ntotal),2)\n","print(f\"We have {np.sum(groups==0)} in one group and {np.sum(groups==1)} in the other\")\n","ncombos=factorial(ntotal) / (factorial(npergroup) * factorial(ntotal-npergroup))\n","print(f\"We then have a total of {ncombos:.0f} combinations\")\n","\n","all_combos=list(itertools.combinations(groups,npergroup))\n","print(f\"Or, computed another way ... (hint hint) {len(all_combos)} combinations\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vydY-uC0nygD"},"source":["# Get the data\n","For this next bit, we're going to use some datasets I've already made. I used my code for `GenNormalSamples` to make up some data for us, so that we're all working on the same input. I've got a few ways you can get / load the data for this.  First, you can just download it from the class [GitHub repository](https://github.com/celstark/n247data). It's the Orig*.csv set there.  If you're working in a local Jupyter notebook, this may be the easiest way. Stick that in your folder here and you can use the normal Pandas functions to read CSV files just fine.\n","\n","Second, though, you can save these files into a Google Drive and load them from here. This is particularly relevant if you're running things on Google Colab.  Third, you can pull them straight from the GitHub repo as you run the code.  \n","\n","Samples of how to do the last two options are here. Note `np.loadtxt` and `np.genfromtxt` only differ in how they handle missing values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2103,"status":"ok","timestamp":1678849469837,"user":{"displayName":"Craig Stark","userId":"11041610993007874858"},"user_tz":420},"id":"gpPUGKDEozMn","outputId":"bc80da7d-a327-490b-fd68-0e33c56d675c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# Loading the files from your Google Drive if you're using Colab and have copied files to your drive (edit as needed to change the actual location)\n","from google.colab import drive\n","import os\n","import numpy as np\n","drive.mount('/content/gdrive')\n","# This can be a personal drive, a shared drive, etc.  Point it to the folder here\n","studydir='/content/gdrive/My Drive/N247_2023'\n","os.chdir(studydir)  \n","\n","OrigSkew=np.loadtxt('OrigSkew.csv')\n","OrigNorm=np.loadtxt('OrigNorm.csv')\n","OrigNullSkew=np.loadtxt('OrigNullSkew.csv')\n","OrigNullNorm=np.loadtxt('OrigNullNorm.csv')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"B7s_oAu2o7Hw"},"outputs":[],"source":["# Loading the files from GitHub directly\n","import numpy as np\n","\n","OrigNorm = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigNorm.csv')\n","OrigSkew = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigSkew.csv')\n","OrigNullNorm = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigNullNorm.csv')\n","OrigNullSkew = np.loadtxt('https://github.com/celstark/n247data/raw/main/data/OrigNullSkew.csv')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ES3BRdGcpuse"},"source":["Now that you've got the data, you're going to write both a permutation test and a Monte Carlo analysis of the data. As described above, permutation tests run all possible permutations of the groupings and are great when that number isn't crazy. When that number gets to be too big, we approximate it via a **Monte Carlo** analysis. This just means, we do a random sampling of those permutations and, if that sampling is really random and of a decent size, we can estimate our proabability just fine.  It sounds fancy, but yea - it's just a random sampling of the permutations.\n","\n","For this next bit, though, we only have 185k possible permutations and this should take just a few seconds to run them all.  So, in this next bit, you'll write a function `PermutationTest` that iterates over all permutations of putting half (10) of the observations in one group and the other half (10) in the other group.  So, for each iteration, compute the mean in each group, compute the difference between these, and store that in an array. If you want this to run before you graduate, make sure to pre-allocate your results array and don't use `append` each time through your loop.\n","\n","At the end, return the percentile for the original group-assignment difference. If, for example, your original difference in means was 5 and there were 2340 values in your \"all differences\" array that were greater than 5, the percentile would be 2340/184756 or 0.0127 when considered \"one-tailed\".  To make this a two-tailed test, you'd take the absolute value of the original difference and count how many times the absolute value of your \"all differences\" array was greater than this, dividing again by the total number of permutations.  (Note, when doing things one-tailed, consider handling positive and negative original differences intelligently).\n","\n","Here are target numbers for the *OrigNullNorm* dataset:\n","```\n","Original difference 0.30\n","t-test p=0.563\n","Permutaton p=0.559\n","```\n","Run on all 4 datasets (*OrigNullNorm*, *OrigNullSkew*, *OrigNorm*, and *OrigSkew*). When getting going, you'll want to comment out all but one of the actual analysis bits.  The numbers above for OrigNullNorm should help you know if you're working. I've included a little plot routine as well that lets you look at the distribution of differences. This is also my plot for the *OrigNullNorm* and *OrigSkew* datasets.\n","\n","| ![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig3a.png)       | ![sample output](https://github.com/celstark/n247data/raw/main/data/stats_perm_fig3b.png)    | \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KQi956E4wOH7"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","import itertools\n","from math import factorial\n","import numpy as np\n","import seaborn as sns\n","\n","def PermutationPlot(all_diffs, orig_diff=None):\n","    # Handy function to plot the output of your PertmutationTest call\n","    g=sns.displot(all_diffs,kind='kde')\n","    if orig_diff:\n","        g.refline(x=orig_diff,color='k')\n","\n","def PermutationTest(data,tails=2,verbose=False):\n","    # Your code here...\n","   \n","    return ptile,orig_diff,all_diffs\n","\n","\n","# Start by commenting out all but one of these analyses until you've got things going.\n","ptile,orig_diff,all_diffs=PermutationTest(OrigNullNorm)\n","print(f'Null, normal p={ptile:.3f}\\n')\n","PermutationPlot(all_diffs,orig_diff)\n","\n","ptile,orig_diff,all_diffs=PermutationTest(OrigNullSkew)\n","print(f'Null effect, skew p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=PermutationTest(OrigNorm)\n","print(f'Effect, normal p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=PermutationTest(OrigSkew)\n","print(f'Effect, skew p={ptile:.3f}\\n')\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6ZVlsPikVWY9"},"source":["# Monte Carlo version\n","Well, 185k isn't so crazy for this kind of thing, but what if we needed a lot more math than just a simple difference? What if each iteration then took several seconds?  Or, what if we have larger samples than two groups of 10?  The time it takes to run a full permutation can get out of hand quickly.\n","\n","Just as we can approximate how a population behaves by sampling it (the fundamental idea in all of statistics), we can approximate how our measure behaves by sampling it rather than by testing every possible point. That's what we do in a Monte Carlo analysis.\n","\n","Here, re-write your code to just randomly shuffle the groups `nshuffles` times (default 1000), compute the difference each time and return the same percentile and \"all differences\" array as before. This should be a plug-in replacement for your `PermutationTest()` routine.  Run on all 4 datasets and you should see the p-values are darn close, even with the default of 1000 samples.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ig2QvyieWgpn"},"outputs":[],"source":["from scipy.stats import ttest_ind\n","import numpy as np\n","\n","def MonteCarloTest(data,nshuffles=1000,tails=2,verbose=False):\n","    # Your code here\n","\n","    return ptile,orig_diff,all_diffs\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigNullNorm)\n","print(f'Null, normal p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigNullSkew)\n","print(f'Null effect, skew p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigNorm)\n","print(f'Effect, normal p={ptile:.3f}\\n')\n","\n","ptile,orig_diff,all_diffs=MonteCarloTest(OrigSkew)\n","print(f'Effect, skew p={ptile:.3f}\\n')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO2KGv9QQtChYiQrTUO5gny","provenance":[{"file_id":"1NFIbl7M1bvmWA_vT9I0sRMQkRzmvRJXt","timestamp":1679007859208}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
